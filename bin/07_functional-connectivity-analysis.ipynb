{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Connectivity Analysis\n",
    "***\n",
    "\n",
    "Now we have an idea of three important components to analyzing neuroimaging data:\n",
    "\n",
    "1. Data manipulation\n",
    "2. Cleaning and confound regression\n",
    "3. Parcellation and signal extraction\n",
    "\n",
    "In this notebook the goal is to integrate these 3 basic components and perform a full analysis of group data using **Intranetwork Functional Connectivity (FC)**. \n",
    "\n",
    "Intranetwork functional connectivity is essentially a result of performing correlational analysis on mean signals extracted from two ROIs. Using this method we can examine how well certain resting state networks, such as the **Default Mode Network (DMN)**, are synchronized across spatially distinct regions. \n",
    "\n",
    "ROI-based correlational analysis forms the basis of many more sophisticated kinds of functional imaging analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "***\n",
    "\n",
    "The outline of the notebook is divided into two parts. The first part directly uses what you've learned and builds upon it to perform the final functional connectivity analysis on group data. \n",
    "\n",
    "The second part shows how we can use Nilearn's convenient wrapper functionality to perform the same task with *significantly less effort*. \n",
    "\n",
    "#### Part A: Manual computation \n",
    "1. Functional data cleaning and confound regression\n",
    "2. Applying a parcellation onto the data\n",
    "3. Computing the correlation between two ROI time-series\n",
    "\n",
    "\n",
    "#### Part B: Using Nilearn's high-level features\n",
    "1. Using NiftiLabelsMasker to extract cleaned time-series\n",
    "2. Computing the correlation between two ROI time-series\n",
    "3. Performing analysis on all subjects\n",
    "4. Visualization of final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nilearn import signal as sgl\n",
    "from nilearn import image as img\n",
    "from nilearn import plotting as plot\n",
    "from nilearn import datasets\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = '10788'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting up directories\n",
    "func_dir = '../data/ds000030/derivatives/fmriprep/sub-{}/func/'.format(sub)\n",
    "\n",
    "#Pre-processed functional data in MNI space\n",
    "func_file = 'sub-{}_task-rest_bold_space-MNI152NLin2009cAsym_preproc.nii.gz'.format(sub)\n",
    "func_path = os.path.join(func_dir,func_file) \n",
    "\n",
    "#Confounds.tsv extracted using FMRIPREP \n",
    "confound_file = 'sub-{}_task-rest_bold_confounds.tsv'.format(sub)\n",
    "confound_path = os.path.join(func_dir,confound_file) \n",
    "\n",
    "#MNI Brainmask outputted from FMRIPREP\n",
    "mask_file = 'sub-{}_task-rest_bold_space-MNI152NLin2009cAsym_brainmask.nii.gz'.format(sub)\n",
    "mask_path = os.path.join(func_dir,mask_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Manual Computation of Functional Connectivity\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Cleaning your functional data using filtering, dummy TR removal and confound regression\n",
    "The first step to any functional analysis is to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to help extract our confound regressors from the .tsv file for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Refer to part_06 for code + explanation\n",
    "def extract_confounds(confound_tsv,confounds,dt=True):\n",
    "    '''\n",
    "    Arguments:\n",
    "        confound_tsv                    Full path to confounds.tsv\n",
    "        confounds                       A list of confounder variables to extract\n",
    "        dt                              Compute temporal derivatives [default = True]\n",
    "        \n",
    "    Outputs:\n",
    "        confound_mat                    \n",
    "    '''\n",
    "    \n",
    "    #Load in data using Pandas then extract relevant columns\n",
    "    confound_df = pd.read_csv(confound_tsv,delimiter='\\t') \n",
    "    confound_df = confound_df[confounds]\n",
    "    \n",
    "    #If using temporal derivatives \n",
    "    if dt:\n",
    "        #For each column create a new column '<colname>_dt' containing the step-wise differences\n",
    "        for col in confound_df.columns:\n",
    "            confound_df['{}_dt'.format(col)] = confound_df[col].diff() \n",
    "    \n",
    "    #Convert into a matrix of values (timepoints)x(variable)\n",
    "    confound_mat = confound_df.values \n",
    "    \n",
    "    #Return confound matrix\n",
    "    return confound_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load functional image\n",
    "\n",
    "#Extract confounds + TR drop\n",
    " \n",
    "#Clean functional image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setting up the parcellation scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply a parcellation we'll have to specify a parcellation to use. \n",
    "For this analysis we'll be using a spatially separated version [Yeo 2011 - 7 Networks](https://www.ncbi.nlm.nih.gov/pubmed/21653723).\n",
    "\n",
    "We chose this parcellation since it nicely characterizes the **DMN**, our network of interest.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeo_atlas = img.load_img('../resources/rois/yeo_2011/Yeo_JNeurophysiol11_MNI152/relabeled_yeo_atlas.nii.gz') \n",
    "\n",
    "#Let's visualize it\n",
    "plot.plot_roi(yeo_atlas,cmap='Paired')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in order to use the parcellation with our functional data it must have the same dimensions. It turns out that the parcellation schema has slightly different dimensions, so we need to resample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We use \"nearest\" to preserve the label values (a label of 2.42 for example, doesn't and shouldn't exist) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Intra-network functional connectivity analysis\n",
    "Intra-network functional connectivity analysis is determined by computing the correlation between the mean time-series of two spatially distinct regions within the same network. \n",
    "\n",
    "To perform this analysis requires a few simple steps:\n",
    "1. Select 2 ROIs from the same network (DMN) \n",
    "2. Extract the mean time-series from both regions \n",
    "3. Compute the correlation between the two mean ROI time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to select two ROIs from the DMN. We've already gone through the hassle of selecting these two regions but many possible combinations exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select two ROIs and visualize\n",
    "source_ROI = 44\n",
    "target_ROI = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the ROI, we can do this by masking our all values not matching our ROI\n",
    " \n",
    "\n",
    "#Apply the mask to the data\n",
    "\n",
    "\n",
    "#Visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do the same for the target ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our two regions selected, we'll now extract the mean time-series for each of our two ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Recall that in the parcellation, each voxel is labelled with a number corresponding to a distinct parcel\n",
    "#We want to extract parcels belonging to our ROI.\n",
    "\n",
    "#Pull the voxels indices belonging to the ROI \n",
    "\n",
    "\n",
    "#Get voxel coordinates (x,y,z) list of source and target ROIs\n",
    "\n",
    "\n",
    "#Load up functional data to extract ROI voxels from\n",
    "\n",
    "\n",
    "#Extract the list of voxel time-series belonging to each ROI\n",
    "#This is now a (roi voxel)x(timepoints) array\n",
    "\n",
    "\n",
    "#We want to compute the mean timeseries of each list of voxels (source and target) \n",
    "#This will be a (1) x (timepoints) vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've extracted two average time-series, one from the source, and one from the target.\n",
    "The last step is to compute the correlation between them. This will tell us how well the average time-series from the two DMN regions synchronize with each other.\n",
    "\n",
    "To do this we use <code>np.corrcoef(x,y)</code> which returns a matrix of form:\n",
    "\n",
    "$$ \\rho=\n",
    "\\left( \\begin{matrix}\n",
    "\\rho_{1,1} & \\rho_{1,2} \\\\\n",
    "\\rho_{2,1} & \\rho_{2,2}\n",
    "\\end{matrix} \\right)\n",
    "$$\n",
    "\n",
    "The diagonals represent the correlation of signals with themselves. These are always $1$. The off-diagonal represents the correlation of one signal with another is exactly what we want. In addition the matrix is symmetric so: $\\rho_{1,2} = \\rho_{2,1}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute correlation and pull the value in the first row, second column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Using nilearn's label masker to extract the timeseries\n",
    "***\n",
    "nilearn has a built in function for extracting timeseries from functional files and doing all the extra signal processing at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nilearn import input_data\n",
    "\n",
    "masker = input_data.NiftiLabelsMasker(labels_img=yeo_atlas,\n",
    "                                      standardize=True,\n",
    "                                      memory='nilearn_cache',\n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nilearn's ConnectivityMeasure to calculate our correlation matrix\n",
    "\n",
    "The avalable options are “correlation”, “partial correlation”, “tangent”, “covariance”, “precision” or other utilites in sci-py could be plugged in ([see here for an example](http://nilearn.github.io/auto_examples/03_connectivity/plot_multi_subject_connectome.html#sphx-glr-auto-examples-03-connectivity-plot-multi-subject-connectome-py)).\n",
    "\n",
    "Let's do correlation this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctrl_correlation_matrices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the full set of connectivity values across multiple ROIs, we'd like to pull matrices specific to the ROI's of interest (44,46). \n",
    "We'll first pull the set of intra-network connectivities for our two ROIs of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "connectivity_metric = 'correlation'\n",
    "connectivity_measure = ConnectivityMeasure(kind=connectivity_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = bids.layout.BIDSLayout('../data/ds000030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = layout.get_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_subjects = []\n",
    "ctrl_subjects = []\n",
    "schz_subjects = []\n",
    "\n",
    "for sub in subjects:\n",
    "    func_file = layout.get(subject='10171', modality='func', type='preproc', extensions='nii.gz', return_type='file')[0]\n",
    "    confound_file = layout.get(subject='10171', modality='func', type='confounds', return_type='file')[0]\n",
    "    func_img = img.load_img(func_file)\n",
    "    func_img = func_img.slicer[:,:,:,5:]\n",
    "    confounds = extract_confounds(confound_file,['X','Y','Z','RotX','RotY','RotZ','GlobalSignal'])\n",
    "    confounds = confounds[5:,:] \n",
    "    time_series = masker.fit_transform(func_img, confounds)\n",
    "    pooled_subjects.append(time_series)\n",
    "    if sub.startswith('1'):\n",
    "        ctrl_subjects.append(time_series)\n",
    "    if sub.startswith('5'):\n",
    "        schz_subjects.append(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn visualization is best achieved using a Pandas dataframe (much like R's dataframes). A dataframe is essentially a spreadsheet-like table and is easily created using numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation_measure = ConnectivityMeasure(kind='correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_correlation_matrices = correlation_measure.fit_transform(ctrl_subjects)\n",
    "schz_correlation_matrices = correlation_measure.fit_transform(schz_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_correlation_matrix = correlation_measure.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from nilearn import plotting\n",
    "\n",
    "def plot_matrices(matrices, matrix_kind):\n",
    "    n_matrices = len(matrices)\n",
    "    fig = plt.figure(figsize=(n_matrices * 4, 4))\n",
    "    for n_subject, matrix in enumerate(matrices):\n",
    "        plt.subplot(1, n_matrices, n_subject + 1)\n",
    "        matrix = matrix.copy()  # avoid side effects\n",
    "        # Set diagonal to zero, for better visualization\n",
    "        np.fill_diagonal(matrix, 0)\n",
    "        vmax = np.max(np.abs(matrix))\n",
    "        title = '{0}, subject {1}'.format(matrix_kind, n_subject)\n",
    "        plotting.plot_matrix(matrix, vmin=-vmax, vmax=vmax, cmap='RdBu_r',\n",
    "                             title=title, figure=fig, colorbar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_matrices(ctrl_correlation_matrices, 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrices(schz_correlation_matrices, 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_mean_correlation_matrix = ctrl_correlation_matrices.mean(axis=0)\n",
    "schz_mean_correlation_matrix = schz_correlation_matrices.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_mean_correlation_matrix[43, 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schz_mean_correlation_matrix[43,45]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
